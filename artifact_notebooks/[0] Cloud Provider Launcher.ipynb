{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb34f9e7-c274-4238-ad78-7126a6106ad7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72118c6-832f-4c71-bacf-ea4ee8f9e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180db668-4e11-47dc-94e0-0c8f2b18dd0e",
   "metadata": {},
   "source": [
    "## System Configuration Files\n",
    "\n",
    "The system configuration file variables are organized in the `system_cfg_files_{# GPUs}{,_2,_3}` format, where\n",
    "- `{# GPUs}` is the # of GPUs in the target system cluster\n",
    "- `{,_2,_3}` is used to further organize clusters based on the number of devices in each node (noted by the `# Node Size ...` comment). This will be important for customizing the parallelization strategies used in `task_cfgs/cloud/dlrm_train_cloud.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292ee6ae-28d3-4e04-bd9f-223ab1562e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_cfg_files_64 = [\n",
    "    'system_cfgs/cloud_A/cloud_A_a100.80_64.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_h100.80_64.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.40_64.json',\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40_64.json',\n",
    "]\n",
    "\n",
    "# Node Size 4\n",
    "system_cfg_files_64_2 = [\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.80_64.json',\n",
    "]\n",
    "\n",
    "system_cfg_files_128 = [\n",
    "    'system_cfgs/cloud_A/cloud_A_a100.40_128.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_h100.80_128.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.40_128.json',\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40_128.json',\n",
    "]\n",
    "\n",
    "# Node Size 4\n",
    "system_cfg_files_128_2 = [\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.80_128.json',\n",
    "]\n",
    "\n",
    "# Node Size 16\n",
    "system_cfg_files_128_3 = [\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40m_128.json',\n",
    "]\n",
    "\n",
    "system_cfg_files_256 = [\n",
    "    'system_cfgs/cloud_A/cloud_A_v100.32_256.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_a100.40_256.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_h100.80_256.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_v100.32_256.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.40_256.json',\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40_256.json',\n",
    "]\n",
    "\n",
    "# Node Size 4\n",
    "system_cfg_files_256_2 = [\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.80_256.json',\n",
    "]\n",
    "\n",
    "# Node Size 16\n",
    "system_cfg_files_256_3 = [\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40m_256.json',\n",
    "]\n",
    "\n",
    "system_cfg_files_512 = [\n",
    "    'system_cfgs/cloud_A/cloud_A_v100.16_512.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_v100.32_512.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_a100.40_512.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_h100.80_512.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_v100.32_512.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.40_512.json',\n",
    "    'system_cfgs/cloud_C/cloud_C_v100.16_512.json',\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40_512.json',\n",
    "]\n",
    "\n",
    "# Node Size 4\n",
    "system_cfg_files_512_2 = [\n",
    "    'system_cfgs/cloud_B/cloud_B_v100.16_512.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.80_512.json',\n",
    "]\n",
    "\n",
    "# Node Size 16\n",
    "system_cfg_files_512_3 = [\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40m_512.json',\n",
    "]\n",
    "\n",
    "system_cfg_files_1024 = [\n",
    "    'system_cfgs/cloud_A/cloud_A_v100.16_1024.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_v100.32_1024.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_a100.40_1024.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_h100.80_1024.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_v100.32_1024.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.40_1024.json',\n",
    "    'system_cfgs/cloud_C/cloud_C_v100.16_1024.json',\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40_1024.json',\n",
    "]\n",
    "\n",
    "# Node Size 4\n",
    "system_cfg_files_1024_2 = [\n",
    "    'system_cfgs/cloud_B/cloud_B_v100.16_1024.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.80_1024.json',\n",
    "]\n",
    "\n",
    "# Node Size 16\n",
    "system_cfg_files_1024_3 = [\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40m_1024.json',\n",
    "]\n",
    "\n",
    "system_cfg_files_2048 = [\n",
    "    'system_cfgs/cloud_A/cloud_A_v100.16_2048.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_v100.32_2048.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_a100.40_2048.json',\n",
    "    'system_cfgs/cloud_A/cloud_A_h100.80_2048.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_v100.32_2048.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.40_2048.json',\n",
    "    'system_cfgs/cloud_C/cloud_C_v100.16_2048.json',\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40_2048.json',\n",
    "]\n",
    "\n",
    "# Node Size 4\n",
    "system_cfg_files_2048_2 = [\n",
    "    'system_cfgs/cloud_B/cloud_B_v100.16_2048.json',\n",
    "    'system_cfgs/cloud_B/cloud_B_a100.80_2048.json',\n",
    "]\n",
    "\n",
    "# Node Size 16\n",
    "system_cfg_files_2048_3 = [\n",
    "    'system_cfgs/cloud_C/cloud_C_a100.40m_2048.json',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d0171-4e83-42d0-aa22-df727b6cbc98",
   "metadata": {},
   "source": [
    "## Main Run Script\n",
    "\n",
    "Run this to emulate training performance on target cloud instance clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e3e85f-b199-4f12-a414-31f0bbae1bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Model Name: DLRM_A\n",
      "Parameters: 801.17 B (0.04% dense, 99.96% sparse).\n",
      "Size: 1602.98 GB (1.28 GB dense, 1601.70 GB sparse).\n",
      "FLOPs: 638.08 MFLOPs (31.90 MFLOPs per MLP layer) per sample.\n",
      "Lookup Bytes: 11.55 MB per sample.\n",
      "**************************************************\n",
      "**************************************************\n",
      "System Name: CloudA-128-p4d.24xlarge\n",
      "16 nodes with 8 devices each\n",
      "Effective FLOPs:\n",
      "\tFP64: 6.84 TFLOPS per device / 0.88 PFLOPS system-wide\n",
      "\tFP/TF32: 109.98 TFLOPS per device / 14.08 PFLOPS system-wide\n",
      "\tFP/BF16: 219.96 TFLOPS per device / 28.15 PFLOPS system-wide\n",
      "\tINT8: 439.92 TOPS per device / 56.31 POPS system-wide\n",
      "Memory:\n",
      "\tCapacity: 40.00 GB per device / 5.12 TB system-wide\n",
      "\tBandwidth: 1555.00 GB/s per device / 199.04 TB/s system-wide\n",
      "Effective Unidirectional BW per GPU:\n",
      "\tIntra-Node: 135.00 GB/s\n",
      "\tInter-Node: 1.94 GB/s\n",
      "Effective Communication Collectives BW:\n",
      "\tAll to All: 1.94 GB/s\n",
      "\tAll Reduce: 21.43 GB/s\n",
      "\tAll Gather: 42.86 GB/s\n",
      "\tReduce Scatter: 42.86 GB/s\n",
      "**************************************************\n",
      "**************************************************\n",
      "Task Type: pretrain\n",
      "Task Memory Usage:\n",
      "\tModel Weights: 12.52 (0.01 MLP, 12.51 EMB) GB per device.\n",
      "Task FLOPs:\n",
      "\tMLP Layer FLOPs per local batch: 0.02 TFLOPs.\n",
      "\tMLP Layer FLOPs per global batch: 2.09 TFLOPs.\n",
      "\tModel FLOPs per local batch: 0.33 TFLOPs.\n",
      "\tModel FLOPs per global batch: 41.82 TFLOPs.\n",
      "Task Lookup Bytes:\n",
      "\tLookup bytes per local batch: 756.99 GB\n",
      "\tLookup bytes per global batch: 756.99 GB\n",
      "\tLookup bytes per device (per global batch): 5.91 GB\n",
      "**************************************************\n",
      "**************************************************\n",
      "Aggregate Compute Times [ms]:\n",
      "\tGEMM: 8.91\n",
      "\tEMB: 9.10\n",
      "Aggregate Communication Times [ms]:\n",
      "\tAll-to-All: 95.39\n",
      "\tAllReduce: 0.00\n",
      "\tAllGather: 59.55\n",
      "\tReduceScatter: 29.77\n",
      "Communication Overlap Breakdown [ms]:\n",
      "\tExposed Communication: 180.40 (97.67 %)\n",
      "\tOverlapped Communication: 4.31 (2.33 %)\n",
      "Task Iteration Time [ms]: 198.41\n",
      "Task Throughput: 0.33 MQPS\n",
      "**************************************************\n",
      "**************************************************\n",
      "Model Name: DLRM_A\n",
      "Parameters: 801.17 B (0.04% dense, 99.96% sparse).\n",
      "Size: 1602.98 GB (1.28 GB dense, 1601.70 GB sparse).\n",
      "FLOPs: 638.08 MFLOPs (31.90 MFLOPs per MLP layer) per sample.\n",
      "Lookup Bytes: 11.55 MB per sample.\n",
      "**************************************************\n",
      "**************************************************\n",
      "System Name: CloudA-128-p5.48xlarge\n",
      "16 nodes with 8 devices each\n",
      "Effective FLOPs:\n",
      "\tFP64: 18.33 TFLOPS per device / 2.35 PFLOPS system-wide\n",
      "\tFP/TF32: 266.49 TFLOPS per device / 34.11 PFLOPS system-wide\n",
      "\tFP/BF16: 532.98 TFLOPS per device / 68.22 PFLOPS system-wide\n",
      "\tINT8: 1066.66 TOPS per device / 136.53 POPS system-wide\n",
      "Memory:\n",
      "\tCapacity: 80.00 GB per device / 10.24 TB system-wide\n",
      "\tBandwidth: 2000.00 GB/s per device / 256.00 TB/s system-wide\n",
      "Effective Unidirectional BW per GPU:\n",
      "\tIntra-Node: 202.50 GB/s\n",
      "\tInter-Node: 15.50 GB/s\n",
      "Effective Communication Collectives BW:\n",
      "\tAll to All: 15.50 GB/s\n",
      "\tAll Reduce: 42.90 GB/s\n",
      "\tAll Gather: 85.79 GB/s\n",
      "\tReduce Scatter: 85.79 GB/s\n",
      "**************************************************\n",
      "**************************************************\n",
      "Task Type: pretrain\n",
      "Task Memory Usage:\n",
      "\tModel Weights: 12.52 (0.01 MLP, 12.51 EMB) GB per device.\n",
      "Task FLOPs:\n",
      "\tMLP Layer FLOPs per local batch: 0.02 TFLOPs.\n",
      "\tMLP Layer FLOPs per global batch: 2.09 TFLOPs.\n",
      "\tModel FLOPs per local batch: 0.33 TFLOPs.\n",
      "\tModel FLOPs per global batch: 41.82 TFLOPs.\n",
      "Task Lookup Bytes:\n",
      "\tLookup bytes per local batch: 756.99 GB\n",
      "\tLookup bytes per global batch: 756.99 GB\n",
      "\tLookup bytes per device (per global batch): 5.91 GB\n",
      "**************************************************\n",
      "**************************************************\n",
      "Aggregate Compute Times [ms]:\n",
      "\tGEMM: 3.68\n",
      "\tEMB: 7.07\n",
      "Aggregate Communication Times [ms]:\n",
      "\tAll-to-All: 11.92\n",
      "\tAllReduce: 0.00\n",
      "\tAllGather: 29.75\n",
      "\tReduceScatter: 14.88\n",
      "Communication Overlap Breakdown [ms]:\n",
      "\tExposed Communication: 54.64 (96.63 %)\n",
      "\tOverlapped Communication: 1.91 (3.37 %)\n",
      "Task Iteration Time [ms]: 65.39\n",
      "Task Throughput: 1.00 MQPS\n",
      "**************************************************\n",
      "**************************************************\n",
      "Model Name: DLRM_A\n",
      "Parameters: 801.17 B (0.04% dense, 99.96% sparse).\n",
      "Size: 1602.98 GB (1.28 GB dense, 1601.70 GB sparse).\n",
      "FLOPs: 638.08 MFLOPs (31.90 MFLOPs per MLP layer) per sample.\n",
      "Lookup Bytes: 11.55 MB per sample.\n",
      "**************************************************\n",
      "**************************************************\n",
      "System Name: CloudB-128-ND96asr_v4\n",
      "16 nodes with 8 devices each\n",
      "Effective FLOPs:\n",
      "\tFP64: 6.84 TFLOPS per device / 0.88 PFLOPS system-wide\n",
      "\tFP/TF32: 109.98 TFLOPS per device / 14.08 PFLOPS system-wide\n",
      "\tFP/BF16: 219.96 TFLOPS per device / 28.15 PFLOPS system-wide\n",
      "\tINT8: 439.92 TOPS per device / 56.31 POPS system-wide\n",
      "Memory:\n",
      "\tCapacity: 40.00 GB per device / 5.12 TB system-wide\n",
      "\tBandwidth: 1555.00 GB/s per device / 199.04 TB/s system-wide\n",
      "Effective Unidirectional BW per GPU:\n",
      "\tIntra-Node: 135.00 GB/s\n",
      "\tInter-Node: 7.75 GB/s\n",
      "Effective Communication Collectives BW:\n",
      "\tAll to All: 7.75 GB/s\n",
      "\tAll Reduce: 26.39 GB/s\n",
      "\tAll Gather: 52.78 GB/s\n",
      "\tReduce Scatter: 52.78 GB/s\n",
      "**************************************************\n",
      "**************************************************\n",
      "Task Type: pretrain\n",
      "Task Memory Usage:\n",
      "\tModel Weights: 12.52 (0.01 MLP, 12.51 EMB) GB per device.\n",
      "Task FLOPs:\n",
      "\tMLP Layer FLOPs per local batch: 0.02 TFLOPs.\n",
      "\tMLP Layer FLOPs per global batch: 2.09 TFLOPs.\n",
      "\tModel FLOPs per local batch: 0.33 TFLOPs.\n",
      "\tModel FLOPs per global batch: 41.82 TFLOPs.\n",
      "Task Lookup Bytes:\n",
      "\tLookup bytes per local batch: 756.99 GB\n",
      "\tLookup bytes per global batch: 756.99 GB\n",
      "\tLookup bytes per device (per global batch): 5.91 GB\n",
      "**************************************************\n",
      "**************************************************\n",
      "Aggregate Compute Times [ms]:\n",
      "\tGEMM: 8.91\n",
      "\tEMB: 9.10\n",
      "Aggregate Communication Times [ms]:\n",
      "\tAll-to-All: 23.85\n",
      "\tAllReduce: 0.00\n",
      "\tAllGather: 48.35\n",
      "\tReduceScatter: 24.18\n",
      "Communication Overlap Breakdown [ms]:\n",
      "\tExposed Communication: 92.35 (95.82 %)\n",
      "\tOverlapped Communication: 4.03 (4.18 %)\n",
      "Task Iteration Time [ms]: 110.36\n",
      "Task Throughput: 0.59 MQPS\n",
      "**************************************************\n",
      "**************************************************\n",
      "Model Name: DLRM_A\n",
      "Parameters: 801.17 B (0.04% dense, 99.96% sparse).\n",
      "Size: 1602.98 GB (1.28 GB dense, 1601.70 GB sparse).\n",
      "FLOPs: 638.08 MFLOPs (31.90 MFLOPs per MLP layer) per sample.\n",
      "Lookup Bytes: 11.55 MB per sample.\n",
      "**************************************************\n",
      "**************************************************\n",
      "System Name: CloudC-128-a2-highgpu-8g\n",
      "16 nodes with 8 devices each\n",
      "Effective FLOPs:\n",
      "\tFP64: 6.84 TFLOPS per device / 0.88 PFLOPS system-wide\n",
      "\tFP/TF32: 109.98 TFLOPS per device / 14.08 PFLOPS system-wide\n",
      "\tFP/BF16: 219.96 TFLOPS per device / 28.15 PFLOPS system-wide\n",
      "\tINT8: 439.92 TOPS per device / 56.31 POPS system-wide\n",
      "Memory:\n",
      "\tCapacity: 40.00 GB per device / 5.12 TB system-wide\n",
      "\tBandwidth: 1555.00 GB/s per device / 199.04 TB/s system-wide\n",
      "Effective Unidirectional BW per GPU:\n",
      "\tIntra-Node: 135.00 GB/s\n",
      "\tInter-Node: 0.48 GB/s\n",
      "Effective Communication Collectives BW:\n",
      "\tAll to All: 0.48 GB/s\n",
      "\tAll Reduce: 20.19 GB/s\n",
      "\tAll Gather: 40.38 GB/s\n",
      "\tReduce Scatter: 40.38 GB/s\n",
      "**************************************************\n",
      "**************************************************\n",
      "Task Type: pretrain\n",
      "Task Memory Usage:\n",
      "\tModel Weights: 12.52 (0.01 MLP, 12.51 EMB) GB per device.\n",
      "Task FLOPs:\n",
      "\tMLP Layer FLOPs per local batch: 0.02 TFLOPs.\n",
      "\tMLP Layer FLOPs per global batch: 2.09 TFLOPs.\n",
      "\tModel FLOPs per local batch: 0.33 TFLOPs.\n",
      "\tModel FLOPs per global batch: 41.82 TFLOPs.\n",
      "Task Lookup Bytes:\n",
      "\tLookup bytes per local batch: 756.99 GB\n",
      "\tLookup bytes per global batch: 756.99 GB\n",
      "\tLookup bytes per device (per global batch): 5.91 GB\n",
      "**************************************************\n",
      "**************************************************\n",
      "Aggregate Compute Times [ms]:\n",
      "\tGEMM: 8.91\n",
      "\tEMB: 9.10\n",
      "Aggregate Communication Times [ms]:\n",
      "\tAll-to-All: 381.55\n",
      "\tAllReduce: 0.00\n",
      "\tAllGather: 63.20\n",
      "\tReduceScatter: 31.60\n",
      "Communication Overlap Breakdown [ms]:\n",
      "\tExposed Communication: 471.95 (99.08 %)\n",
      "\tOverlapped Communication: 4.40 (0.92 %)\n",
      "Task Iteration Time [ms]: 489.96\n",
      "Task Throughput: 0.13 MQPS\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "model_cfg_file = 'model_cfgs/dlrm/dlrm_a.json'\n",
    "task_cfg_file = \"task_cfgs/cloud/dlrm_train_cloud.json\"\n",
    "system_cfg_files = system_cfg_files_128 # change me to the defined system configuration block variables above\n",
    "\n",
    "for system_cfg_file in system_cfg_files:\n",
    "    os.system('python ../run_model.py --model-cfg-file \\'../{}\\' --system-cfg-file \\'../{}\\' --task-cfg-file \\'../{}\\''.format(\n",
    "                    model_cfg_file, system_cfg_file, task_cfg_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba5817f-4b68-4faf-b93f-77eb3928f579",
   "metadata": {},
   "source": [
    "- Note that for the main run script, when you change the system configuration variable to one with a different node size (as indicated by comment line), you may have to change the corresponding task configuration file `\"task_cfgs/dlrm_train_cloud.json\"` as well\n",
    "- The default settings for the code block above will generate task throughputs that match the entries highlighted in yellow in the reference '[Artifact Evaluation] Cloud Provider Results' sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1631f2-a0a3-4d46-a3a2-30dd5d24e350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
